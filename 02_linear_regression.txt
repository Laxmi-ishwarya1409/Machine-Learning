Linear Regression 
    -> Linear Regression is a Supervised Learning algorithm used for predicting a continuous value (Y) from one or more input features (X).

    -> used to predict a continuous value based on 1 or more input variables

    -> Example: Predict a personâ€™s salary (Y) based on years of experience (X)
    -> We expect that: As experience increases, salary increases approximately linearly.

    -> So the relationship between X and Y can be modeled by a straight line:
                Y = mX + c
                where, m = slope(how much Y changes for unit change in X)   
                    formula : m=n(âˆ‘XY)-(âˆ‘X)(âˆ‘Y)/n(âˆ‘XÂ²) - (âˆ‘X)Â²
                c = intercept value 
                    formula : c = âˆ‘Y - mâˆ‘X/n
                X = input variable
                Y= Predicted or output feature
                
    -> Linear Regression finds the best fitting staright line throgh all the points 

    -> Purpose: is to find the relationship between 2 or more variables and then use those variables to make predictions

    -> Goal : To find the line that gives the least error betweenpredicted and actual values 


What Does Best Lineâ€= Mean?

    -> The best line is the one that makes the difference between actual Y and predicted Y as small as possible.

    -> This difference (error) = ğ‘Œâˆ’ğ‘Œ(predicted)

    -> But sometimes errors are negative, sometimes positive,so we square them to make all positive and add them up.

    -> Then we take the average â†’ thatâ€™s called the Mean Squared Error (MSE) or Cost Function.

Detail Explanation:
    step 1: take a traing datasets
    step 2: find the prediction and error 
    step 3: measure total error i.e COST FUNCTION
    step 4: find the mean square error(MSE)
        MSE=1/nâ€‹âˆ‘(Yâˆ’Ypredicted)Â²


        -> This MSE value is also called the Cost Function (J).
                ğ½(ğ‘š,ğ‘)=1/2ğ‘›âˆ‘(ğ‘Œğ‘–âˆ’(ğ‘šğ‘‹ğ‘–+ğ‘))Â² 


        -> if MSE is large - line is not fitting well
        -> if MSE is small - line lis closes to the actual values


    step 5: so we keep on adjusting m and c values
        using gradient desent 

            ---- why derivatives?
            -> to reduce J, we need to know how to change the m and c , So, we do it by partial derivates(gradients)

    step 6: find best m and c (gradient desent)
        -> Goal: Find the minimum value of J(m, c) (the global minimum).

       âˆ‚J/âˆ‚m  - tells how J changes if we do small change or adjustment in m
       âˆ‚J/âˆ‚c - tells how J changes if we do small change or adjustment in c

       We iteratively update m and c:
            m:=mâˆ’Î±(âˆ‚J/âˆ‚m)
            c:=câˆ’Î±(âˆ‚J/âˆ‚c)
       
            where 
                    Î± = learning rate (step size).

    therefore gradients,
        âˆ‚J/âˆ‚m = -2/nâˆ‘Xi(Y input âˆ’Y predicted)
        âˆ‚J/âˆ‚c = -2/nâˆ‘Xi(Y input âˆ’Y predicted)


    Step 7: Visualize Cost Function
        -> If you plot ğ½(ğ‘š) vs ğ‘š, it forms a parabola.
        
        -> The lowest point (valley) is the global minimum, where error is minimum.
        
        ->Thatâ€™s where the line best fits the data.
        
        -> Gradient Descent finds that minimum point step by step by following the slope of the curve.


    step 8: Performace metric

        -> Even after finding the best m and c, we must evaluate how well our model fits unseen data.

            (1) MAE â€“ Mean Absolute Error

                ğ‘€ğ´ğ¸=1/ğ‘›âˆ‘|ğ‘Œ input âˆ’ğ‘Œ preditedâˆ£

                â†’ Gives average magnitude of errors.
                â†’ Easy to interpret, same unit as target variable.

            (2) MSE â€“ Mean Squared Error
                
                ğ‘€ğ‘†ğ¸=1/ğ‘›âˆ‘(ğ‘Œ input âˆ’ğ‘Œ predicted)Â²

                â†’ Penalizes large errors more (since errors are squared).
                â†’ Good for optimization.
                
            (3) RMSE â€“ Root Mean Squared Error

                ğ‘…ğ‘€ğ‘†ğ¸=sqrt(ğ‘€ğ‘†ğ¸)

                â†’ Brings error back to original units of Y.
                â†’ Useful to compare models.
                
            (4) RÂ² â€“ Coefficient of Determination
            
                RÂ² = 1-SSres/ 1- SStot

                where,
                SSres = âˆ‘(ğ‘Œ inputâˆ’ğ‘Œ output)Â²
                SStot = âˆ‘(ğ‘Œ inputâˆ’ğ‘Œbar)Â²

                â†’ Measures how much of the variance in Y is explained by the model.
                â†’ Range: 0 to 1 (closer to 1 = better fit).

            (5) Adjusted RÂ²

                Adj ğ‘…Â²=1âˆ’(1âˆ’ğ‘…Â²)ğ‘›âˆ’1/ğ‘›âˆ’ğ‘âˆ’1

                where ,
                    p = number of predictors.
                    
                â†’ Adjusts RÂ² for multiple features (penalizes extra features that donâ€™t improve the model).


Why We Need These Metrics?
MAE	--- Shows average actual deviation (easy to interpret).
MSE	--- Used for training (smooth gradient).
RMSE --- Same scale as output (practical measure).
RÂ² --- Shows modelâ€™s explanatory power.
Adjusted RÂ² --- Prevents overfitting in multi-variable regression.


Global Minima : lowest point of the parabola (only 1 in linear regression because cost is convex i.e, J decreases)

Local Minima: A lowest point but not the Absolute lowest(non-convex)



Libraries Weâ€™ll Use:
    numpy - Helps do fast math (arrays, sum, mean, etc.)

    pandas - Helps handle tabular data (like Excel tables) i.e, storing in rows and columns
    
    matplotlib - Helps draw graphs

    sklearn - Has ready-made machine learning models (like Linear Regression)






Functions used:
    plt.figure() -- Creates a new plotting area
    plt.plot() -- Draws a line for data points
    plt.title() -- Adds a title to the chart
    plt.xlabel() -- Adds label to X-axis
    plt.ylabel() -- Adds label to Y-axis
    plt.legend() -- Displays which line represents what
    plt.grid(True) -- Adds background grid
    plt.show() -- Renders and displays the chart