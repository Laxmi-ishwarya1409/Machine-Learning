->The goal of linear regression is to find a straight line:
                 ð‘¦=ð‘šð‘‹+ð‘

-> It does this by minimizing the Cost Function (MSE):
                ð½=1/ð‘›âˆ‘(ð‘¦ð‘–âˆ’y predicted)Â²

-> But sometimes, when we have:

        too many features (independent variables)
        noisy or correlated data

-> the model overfits â€” it performs great on training data but poorly on unseen data.






Overfitting & Underfitting

    -> In machine learning, when we train a regression model (like Linear Regression), we want it to work well on new unseen data, not just on the training data.

    âž¤ Overfitting:

        -> Model fits training data too perfectly â€” passes through almost every point.

        -> It performs great on training data but bad on new/test data.

        -> So we say:
                Low bias (fits training data well)

                High variance (fails on new data)

    âž¤ Underfitting:

        -> Model cannot even fit the training data properly.

        -> It performs bad on both training and test data.

        -> So,we say:
                High bias

                High variance

âž¤ Goal:

    -> We want a generalized model â†’ performs well on both training and test data.






Regularization (Ridge & Lasso)

    -> Regularization is a technique to prevent overfitting.

    -> It slightly penalizes large coefficients (slope values) in the model so that the line doesnâ€™t become too steep.



Without Regularization:
    -> The model only cares about minimizing error:

            ð½=1/2ð‘šâˆ‘(ð‘¦ ð‘–âˆ’ð‘¦ predicted)2

    -> This can make the model too flexible.

With Regularization
    -> We tell the model: Donâ€™t just minimize error â€” also keep your coefficients small

    -> So we add a penalty term:
        
        ð½=1/2ð‘šâˆ‘(ð‘¦ ð‘– âˆ’ ð‘¦ predicted)2 + ðœ† x Penalty

        Here:
            ðœ† = Regularization strength/ the hyper parameter : (how much penalty you give i.e, how fast lessesn the steepness and grow the steepness)

            Penalty = depends on L1 or L2

    Role of Î» (Lambda) 

        -> Î» = 0 â†’ no regularization (normal linear regression)

        -> Î» = large â†’ too much penalty (underfitting)

        -> Î» = small but > 0 â†’ just right (balances fit & simplicity)















There are two main types:
1.Ridge Regression â†’ L2 Regularization

    -> Ridge Regression adds the sum of squares of coefficients (weights) to the cost function.
            ð½=1/ð‘›âˆ‘(ð‘¦ ð‘– âˆ’ ð‘¦ predicted)Â² + ðœ† âˆ‘ð‘¤ð‘—Â² 
 
            where:
                ðœ† = regularization strength (hyperparameter)
                ð‘¤ð‘— = model coefficients (slopes)
                ðœ†âˆ‘ð‘¤ð‘—Â² = L2 penalty term

            -> Intuition:

                -> Keeps all features but shrinks coefficients toward 0.

                -> Reduces the impact of less important features.

                -> Helps when thereâ€™s multicollinearity (correlated features).

    Therefore Ridge says : Donâ€™t remove any feature, but keep their weights small and balanced.
    Main Purpose: Prevents Overfitting



2. Lasso Regression (L1 Regularization)

    -> Lasso Regression adds the sum of absolute values of coefficients to the cost function.
            ð½=1/ð‘›âˆ‘(ð‘¦ð‘–âˆ’ð‘¦ predicted)Â²+ðœ†âˆ‘âˆ£ð‘¤ð‘—âˆ£
            
            
    Intuition:

        -> Can force some coefficients to become exactly 0.

        -> This means it automatically selects important features and eliminates irrelevant ones.
        
        -> Great for feature selection.

    Therefore Lasso says: Iâ€™ll keep only the most important features and make the rest 0."

    Main Purpose: Prevents Overfitting and good feature selection