# Regularization in Linear Regression (Ridge & Lasso)

---

## The Goal of Linear Regression

The goal of linear regression is to find a straight line:

\[
y = mX + c
\]

It does this by minimizing the **Cost Function (MSE)**:

\[
ğ½=1/ğ‘›âˆ‘(ğ‘¦ğ‘–âˆ’y predicted)Â²0
\]

But sometimes, when we have:
- Too many features (independent variables)
- Noisy or correlated data

â†’ The model **overfits** â€” it performs great on training data but poorly on unseen data.

---

## Overfitting & Underfitting

When training a regression model (like Linear Regression), we want it to work well on **unseen data**, not just the training data.

### â¤ Overfitting
- Model fits training data **too perfectly** â€” passes through almost every point.
- Performs great on training data but **bad on test data**.
- Therefore:
  - **Low bias** (fits training data well)
  - **High variance** (fails on new data)

### â¤ Underfitting
- Model cannot even fit the training data properly.
- Performs poorly on both training and test data.
- Therefore:
  - **High bias**
  - **High variance**

### â¤ Goal
We want a **generalized model** that performs well on both training and test data.

---

## Regularization (Ridge & Lasso)

Regularization is a technique to **prevent overfitting**.

It **penalizes large coefficients** (slope values) in the model so that the line doesnâ€™t become too steep.

---

### Without Regularization
The model only cares about minimizing error:

\[
ğ½=1/2ğ‘šâˆ‘(ğ‘¦ğ‘–âˆ’ğ‘¦ predicted)Â²
\]

â†’ This can make the model too flexible and overfit.

---

### With Regularization
We tell the model:  
â€œDonâ€™t just minimize error â€” also keep your coefficients small.â€

So, we add a penalty term:

\[
ğ½=1/2ğ‘šâˆ‘(ğ‘¦ ğ‘– âˆ’ ğ‘¦ predicted)Â² + ğœ† x Penalty
\]

Where:
- **Î» (lambda)** = Regularization strength (hyperparameter)  
  â†’ Controls how strong the penalty is.  
- **Penalty** = Depends on whether we use **L1** or **L2** regularization.

---

### Role of Î» (Lambda)

| Î» Value | Meaning | Effect |
|----------|----------|--------|
| Î» = 0 | No regularization | Normal Linear Regression |
| Î» â†’ Large | Too much penalty | Underfitting |
| Î» â†’ Small but > 0 | Just right | Balances fit & simplicity |

---

## Types of Regularization

### 1ï¸âƒ£ Ridge Regression (L2 Regularization)

Ridge Regression adds the **sum of squares of coefficients** (weights) to the cost function:

\[
ğ½=1/ğ‘›âˆ‘(ğ‘¦ ğ‘– âˆ’ ğ‘¦ predicted)Â² + ğœ† âˆ‘ğ‘¤ğ‘—Â² 
\]

Where:
- **Î»** = Regularization strength (hyperparameter)
- **wâ±¼** = Model coefficients (slopes)
- **Î» âˆ‘ wâ±¼Â²** = L2 penalty term

#### ğŸ”¹ Intuition
- Keeps **all features**, but **shrinks coefficients** toward 0.  
- Reduces the impact of less important features.  
- Helps when features are **correlated (multicollinearity)**.

âœ… **Ridge says:**  
> â€œDonâ€™t remove any feature, but keep their weights small and balanced.â€

**Main Purpose:** Prevents Overfitting.

---

### 2ï¸âƒ£ Lasso Regression (L1 Regularization)

Lasso Regression adds the **sum of absolute values of coefficients** to the cost function:

\[
ğ½=1/ğ‘›âˆ‘(ğ‘¦ğ‘–âˆ’ğ‘¦ predicted)Â²+ğœ†âˆ‘âˆ£ğ‘¤ğ‘—âˆ£
\]

#### ğŸ”¹ Intuition
- Can force some coefficients to become **exactly 0**.  
- Automatically selects **important features** and removes irrelevant ones.  
- Great for **feature selection**.

âœ… **Lasso says:**  
> â€œIâ€™ll keep only the most important features and make the rest 0.â€

**Main Purpose:** Prevents Overfitting **and** performs Feature Selection.

---

