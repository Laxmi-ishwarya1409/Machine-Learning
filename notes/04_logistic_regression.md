# Logistic Regression

## What is Logistic Regression?
Logistic Regression is a machine learning algorithm used for **classification problems** â€” where the output is categorical (Yes/No, 0/1, etc.).

---

## Purpose of Logistic Regression
To model the relationship between independent variables (features) and a **binary dependent variable (target)** using probabilities.

---

## Why Logistic Regression?

We use Linear Regression to predict continuous values, but it fails for classification problems because:

- Linear Regression outputs values beyond 0â€“1 (like -3, 5, 10).
- Itâ€™s sensitive to outliers, shifting the line and giving wrong predictions.
- We need probabilities between 0 and 1 for classification.
- Logistic Regression â€œsquashesâ€ linear outputs between 0 and 1 using the **sigmoid function**.

---

## Logistic Regression Equation

We calculate:
\[
z = ğœƒ0+ğœƒ1ğ‘¥1+ğœƒ2ğ‘¥2+â‹¯+ğœƒğ‘›ğ‘¥ğ‘›
\]

Then apply the sigmoid function:
\[
â„ğœƒ(ğ‘¥)=1/1+e^{-z}
\]

So, `h(x)` = probability that the output belongs to class 1.

---

## Sigmoid (Squashing) Function
To squash the linear function output into the [0,1] range, we use the sigmoid (logistic) function
\[
Ïƒ(z)=1/1+e^{-z}
\]

- If z â†’ +âˆ â†’ output â†’ 1  
- If z â†’ -âˆ â†’ output â†’ 0  
- If z = 0 â†’ output = 0.5  

So, it perfectly represents probabilities.

Example:  
z = 2 â†’ 0.88  
z = -2 â†’ 0.12

---

## Decision Boundary
The model decides:

- If â„ğœƒ(ğ‘¥)â‰¥0.5â‡’ğ‘¦=1
- If â„ğœƒ(ğ‘¥)<0.5â‡’ğ‘¦=0

The â€œ0.5 lineâ€ is called the **decision boundary**.

---

## Assumptions
- Dependent variable is binary.  
- Independent variables are linearly related to log-odds of the outcome. 
- Observations are independent.  
- No perfect multicollinearity.  
- The dataset should be large enough to estimate probabilities accurately.

---

## Why Not Linear Regression Cost Function?

Linear regression cost:
\[
ğ½(ğœƒ)=1/2ğ‘šâˆ‘(â„ğœƒ(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))Â²
\]

Fails for logistic regression because:
- Sigmoid introduces non-linearity.
- Cost function becomes non-convex.
- Gradient descent may get stuck in local minima.

---

## Logistic Regression Cost Function

\[
ğ½(ğœƒ)=âˆ’1/ğ‘šâˆ‘ğ‘–=1/ğ‘š[ğ‘¦(ğ‘–)log(â„ğœƒ(ğ‘¥(ğ‘–)))+(1âˆ’ğ‘¦(ğ‘–))log(1âˆ’â„ğœƒ(ğ‘¥(ğ‘–)))]
\]

âœ… **Properties**
- Convex â†’ only one global minimum.  
- Strong penalty for wrong confident predictions.

---

## Gradient Descent for Logistic Regression

We minimize \( ğ½(ğœƒ) \) using:
\[
ğœƒğ‘—:=ğœƒğ‘—âˆ’ğ›¼âˆ‚ğ½(ğœƒ)/âˆ‚ğœƒğ‘—
\]

Where, ğ›¼ = learning rate.

- The gradient (derivative) simplifies nicely to:
\[
âˆ‚ğ½(ğœƒ)/âˆ‚ğœƒğ‘—=1/ğ‘šâˆ‘(â„ğœƒ(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))ğ‘¥ğ‘—(ğ‘–)
\]
- Same as linear regression but with sigmoid instead of plain linear output.

---

## Performance Metrics
-Once the Logistic Regression model predicts probabilities,
we need to evaluate how good those predictions are.

### 1. Confusion Matrix
- A confusion matrix compares actual vs predicted values. It helps us understand how many predictions were correct or wrong.

| Actual / Predicted | Predicted: 1 | Predicted: 0 |
|--------------------|--------------|--------------|
| Actual: 1          | TP           | FN           |
| Actual: 0          | FP           | TN           |

- **TP:** Correctly predicted 1  
- **TN:** Correctly predicted 0  
- **FP:** Predicted 1 but actually 0  
- **FN:** Predicted 0 but actually 1  

---

### 2. Accuracy
\[
Accuracy = {TP + TN}/{TP + TN + FP + FN}
\]
- Best when data is balanced.

---

### 3. Precision
\[
Precision = {TP}/{TP + FP}
\]
- Best when **false positives are costly** (e.g., spam detection).

---

### 4. Recall (Sensitivity / TPR)
\[
Recall = {TP}/{TP + FN}
\]
Best when **false negatives are costly** (e.g., disease detection).

---

5. F1-Score
\[
F1 = 2 * {Precision /times Recall}/{Precision + Recall}
\]
- Best when data is **imbalanced**.

---

### 6. FÎ²-Score
\[
ğ¹ğ›½=(1+ğ›½Â²)Ã—{ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›Ã—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™}/{(ğ›½Â²Ã—ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›)+ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™}
\]

- Î² > 1 â†’ more focus on Recall  
- Î² < 1 â†’ more focus on Precision  

---

### 7. ROC Curve
- Plots **True Positive Rate (Recall)** vs **False Positive Rate (FPR)**
\[
FPR = {FP}/{FP + TN}
\]
- Closer to top-left â†’ better model.

---

### 8. AUC (Area Under ROC Curve)
- Measures the overall ability to distinguish between classes.
- AUC = 1 â†’ Perfect classifier  
- AUC = 0.5 â†’ Random guessing  

---

## Balanced vs Imbalanced Data

- **Balanced:** Equal 0s and 1s â†’ Accuracy works well.  
- **Imbalanced:** One class dominates â†’ Accuracy misleading.

---

## Handling Imbalanced Data
1. **Oversampling:** Duplicate or synthesize minority samples.  
2. **Undersampling:** Remove majority samples.  
3. **Use Proper Metrics:** Precision, Recall, F1, ROC-AUC.  
4. **Adjust Threshold:** Change 0.5 cutoff to favor recall or precision.

---
